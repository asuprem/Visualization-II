{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Intro\n",
    "**ModelTrainer** trains all the models found in the *model_type*'s json file (i.e. *cifar.json*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, pdb, json, re, vis_utils\n",
    "# Import codes to obtain requisite commands\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras.models import Sequential, load_model, Model, model_from_json\n",
    "from keras.layers import Dense, Flatten, Dropout, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Loaders\n",
    "1. **model_json**: This contains model details. The format involves a model set (i.e. *cifar*, etc) and relevant details such as image dimensions, training data size, etc\n",
    "\n",
    "2. **top_level**: Lists the top level files (at this time, they are <span style=\"color:#006400\">['jsons', 'activations', 'data', 'max_activations','history', 'tensorlogs', 'weights']</span>)\n",
    "\n",
    "3. **dir_paths**: The relative directory paths to the **top_level** directories, i.e. *cifar/activations*\n",
    "\n",
    "4. **jsons** : The list of *model.json* files, i.e. *cifar.json* within the **cifar/json** folder\n",
    "\n",
    "5. **model_dirs**: The directory that each of the *model.json*'s training and activation data should reside, i.e. **weights/cifar_1/**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_json = vis_utils.json_load('models.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_type = 'cifar'\n",
    "top_level= [_dir for _dir in os.listdir(model_type) if not _dir.startswith('.')]\n",
    "dir_paths = {entry:os.path.join(model_type, entry) for entry in top_level}\n",
    "jsons = os.listdir(dir_paths['jsons'])\n",
    "model_dirs = [os.path.splitext(_item)[0] for _item in jsons]\n",
    "train_data_dir = 'data/' + model_type + '/train'\n",
    "test_data_dir = 'data/' + model_type + '/test'\n",
    "class_name = {i:str(model_json[model_type]['labels'][i]) for i in range(len(model_json[model_type]['labels']))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Image Data generator\n",
    "Generate the batched keras image data files through **ImageDataGenerator**. Sets up the training and test data, using information from **model_json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 images belonging to 10 classes.\n",
      "Found 10000 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen  = ImageDataGenerator(rescale=1./255)\n",
    "batch_size = model_json[model_type]['batch']\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(model_json[model_type]['size']['width'],model_json[model_type]['size']['height']),\n",
    "        batch_size=batch_size,\n",
    "        classes=[str(_item) for _item in model_json[model_type]['labels']],\n",
    "        class_mode='categorical')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_data_dir,\n",
    "        target_size=(model_json[model_type]['size']['width'],model_json[model_type]['size']['height']),\n",
    "        batch_size=batch_size,\n",
    "        classes=[str(_item) for _item in model_json[model_type]['labels']],\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Training</span>\n",
    "Trains every model stored in *model.json* i.e. *cifar.json* - *cifar_1.json*, *cifar_2.json*, etc. The relevant data are stored in:\n",
    "1. **history**: The model histories (Keras based) are stored in *model_type/history/model.csv* aka **cifar/history/cifar_1.csv**\n",
    "2. **Tensorboard**: The Tensorboard data is stored in **cifar/tensorlogs/cifar_1/**\n",
    "3. **Epoch Weights**: The epoch weights are stored in **cifar/weights/cifar_1/** as *weights-#.hdf5*\n",
    "3. **Best Model**: The best model's weights are stored in **cifar/weights/cifar_1/** as *best.hdf5*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sets up what is completed, so we can skip it in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with cifar_6.json\n",
      "Epoch 1/15\n",
      "781/781 [==============================] - 67s - loss: 1.6792 - acc: 0.3777 - val_loss: 1.3673 - val_acc: 0.4955\n",
      "Epoch 2/15\n",
      "781/781 [==============================] - 66s - loss: 1.2690 - acc: 0.5434 - val_loss: 1.1498 - val_acc: 0.5864\n",
      "Epoch 3/15\n",
      "781/781 [==============================] - 66s - loss: 1.0633 - acc: 0.6209 - val_loss: 1.0273 - val_acc: 0.6359\n",
      "Epoch 4/15\n",
      "781/781 [==============================] - 66s - loss: 0.8959 - acc: 0.6802 - val_loss: 0.9419 - val_acc: 0.6614\n",
      "Epoch 5/15\n",
      "781/781 [==============================] - 66s - loss: 0.7670 - acc: 0.7281 - val_loss: 0.9374 - val_acc: 0.6749\n",
      "Epoch 6/15\n",
      "781/781 [==============================] - 66s - loss: 0.6241 - acc: 0.7794 - val_loss: 0.9774 - val_acc: 0.6683\n",
      "Epoch 7/15\n",
      "781/781 [==============================] - 66s - loss: 0.5115 - acc: 0.8199 - val_loss: 1.0137 - val_acc: 0.6803\n",
      "Epoch 8/15\n",
      "781/781 [==============================] - 66s - loss: 0.4181 - acc: 0.8532 - val_loss: 1.1594 - val_acc: 0.6699\n",
      "Epoch 9/15\n",
      "781/781 [==============================] - 66s - loss: 0.3557 - acc: 0.8736 - val_loss: 1.1461 - val_acc: 0.6828\n",
      "Epoch 10/15\n",
      "781/781 [==============================] - 66s - loss: 0.2881 - acc: 0.9000 - val_loss: 1.2021 - val_acc: 0.6760\n",
      "Epoch 11/15\n",
      "781/781 [==============================] - 66s - loss: 0.2512 - acc: 0.9131 - val_loss: 1.4145 - val_acc: 0.6544\n",
      "Epoch 12/15\n",
      "781/781 [==============================] - 66s - loss: 0.2256 - acc: 0.9231 - val_loss: 1.3312 - val_acc: 0.6678\n",
      "Epoch 13/15\n",
      "781/781 [==============================] - 66s - loss: 0.1940 - acc: 0.9343 - val_loss: 1.3933 - val_acc: 0.6750\n",
      "Epoch 14/15\n",
      "781/781 [==============================] - 66s - loss: 0.1849 - acc: 0.9372 - val_loss: 1.3034 - val_acc: 0.6757\n",
      "Epoch 15/15\n",
      "781/781 [==============================] - 66s - loss: 0.1700 - acc: 0.9423 - val_loss: 1.3592 - val_acc: 0.6756\n",
      "Completed with cifar_6.json\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(os.path.join(dir_paths['supplement'], 'completed.txt')):\n",
    "    _cont = open(os.path.join(dir_paths['supplement'], 'completed.txt'), 'r')\n",
    "    dat = _cont.read().split()\n",
    "    _cont.close()\n",
    "else:\n",
    "    dat = []\n",
    "for idx in range(len(jsons)):\n",
    "    # Setting up the directory structures\n",
    "    if jsons[idx] in dat:\n",
    "        continue\n",
    "    print('Starting with ' + jsons[idx])\n",
    "    dir_list = vis_utils.dir_list_returner(dir_paths, ['weights','tensorlogs','history','supplement'], model_dirs[idx])\n",
    "    #Set up supplemental parameters\n",
    "    compile_params = vis_utils.json_load(os.path.join(dir_list['supplement'], 'compile.json'))\n",
    "    # Loading the Model file\n",
    "    with open(os.path.join(dir_paths['jsons'],jsons[idx]), 'r') as model_file:\n",
    "        model = model_from_json(model_file.read())\n",
    "    model.compile(loss = compile_params['loss'], optimizer = compile_params['optimizer'], metrics=compile_params['metrics'])\n",
    "    # Setting up the callbacks\n",
    "    tensorboard_callback = TensorBoard(log_dir=dir_list['tensorlogs']+'/', histogram_freq=0, write_graph=True, write_images=False)\n",
    "    best_callback = ModelCheckpoint(os.path.join(dir_list['weights'],'best.hdf5'), monitor='val_acc', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "    checkpoint_callback = ModelCheckpoint(os.path.join(dir_list['weights'],'weights-{epoch:02d}.hdf5'), monitor='val_acc', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "    #------------------------------------------------------------------------------------------------\n",
    "    # Train\n",
    "    model_hist = model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=model_json[model_type]['train']/model_json[model_type]['batch'],\n",
    "            epochs = model_json[model_type]['epochs'],\n",
    "            validation_data = test_generator,\n",
    "            validation_steps = model_json[model_type]['test']/model_json[model_type]['batch'],\n",
    "            verbose = 1,\n",
    "            callbacks=[tensorboard_callback, checkpoint_callback, best_callback])\n",
    "    with open(os.path.join(dir_paths['history'], jsons[idx]), 'w') as _compile:\n",
    "            json.dump(model_hist.history, _compile)\n",
    "    print('Completed with ' + jsons[idx])\n",
    "    with open(os.path.join(dir_paths['supplement'], 'completed.txt'), 'a') as _compile:\n",
    "            _compile.write(jsons[idx]+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
